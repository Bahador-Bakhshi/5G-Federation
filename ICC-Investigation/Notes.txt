discount factor = 0 --> ???!!!

generate all possible states --> very big memory

The algorithm is learned over and over for the same set of request --> meaning less, in practice we need training and testing, the final optimal policy is derived from the episods

In this case, QL just learn the best polic for that given set of demands not other cases!!!

Using this approach, at the end, QL learns that the best policy is the greedy policy (which we know!!!!)

Is action depends on request!!!

action = np.argmax(Q[state, :] + np.random.randn(1, tot_actions) * (1 / float(episode + 1))) VS paper

All actions are initialized zero --> when there is an unknown state --> do the greey in the test time --> do a random action

State is INDEPENDET of the request????!!!!!

There is a negative profite if the demand is rejected!!!! (in online KS we don't have such cost)

if the action is local deploy but there is not enought resource in local domain --> federation!

Why normal rand for actions, it should be uniform

Action values was INT insread of FLOAT 

Exploration policy is not known for me, it is uniform in paper and normal in code


